{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b41ada",
   "metadata": {},
   "source": [
    "# Comparaison de la s√©lection de variables avec STABL (Random Forest, XGBoost, Lasso, ElasticNet)\n",
    "\n",
    "Ce notebook compare la s√©lection de variables et les performances de classification/r√©gression entre STABL utilisant des mod√®les d‚Äôarbres (Random Forest, XGBoost) avec grid search, et STABL utilisant des mod√®les lin√©aires (Lasso, ElasticNet). Les r√©sultats sont √©valu√©s par validation crois√©e, en termes de performances et de nombre de variables s√©lectionn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a5d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les biblioth√®ques n√©cessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, ElasticNet\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error\n",
    "from stabl.stabl import Stabl\n",
    "from stabl.adaptive import ALasso, ALogitLasso\n",
    "from stabl.preprocessing import LowInfoFilter\n",
    "# from stabl.visualization import plot_fdr_graph\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.base import clone\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bde822",
   "metadata": {},
   "source": [
    "## 1. Charger et explorer les donn√©es\n",
    "\n",
    "Nous allons charger un jeu de donn√©es tabulaire (exemple : CyTOF.csv et outcome.csv), afficher ses dimensions, types de variables, et quelques statistiques descriptives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Charger les donn√©es\n",
    "\n",
    "features_path = \"/Users/noeamar/Documents/Stanford/data/olivier_data/ina_13OG_df_168_filtered_allstim_new.csv\"\n",
    "outcome_path = \"/Users/noeamar/Documents/Stanford/data/olivier_data/outcome_table_all_pre.csv\"\n",
    "\n",
    "features = pd.read_csv(features_path, index_col=0)\n",
    "outcome = pd.read_csv(outcome_path, index_col=0, dtype={\"DOS\": int})\n",
    "\n",
    "# 2. Garder seulement les patients pr√©sents dans les deux fichiers\n",
    "common_idx = features.index.intersection(outcome.index)\n",
    "features = features.loc[common_idx]\n",
    "outcome  = outcome.loc[common_idx]\n",
    "\n",
    "# 3. Extraire la s√©rie cible\n",
    "y = outcome[\"DOS\"]\n",
    "\n",
    "# 4. Diagnostic rapide\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Outcome shape:\",  outcome.shape)\n",
    "print(\"Features columns:\", features.columns.tolist()[:10], \"‚Ä¶\")\n",
    "print(\"Outcome columns:\",  outcome.columns.tolist())\n",
    "\n",
    "print(\"\\n=== Statistiques descriptives des features ===\")\n",
    "print(features.describe().T)\n",
    "\n",
    "print(\"\\n=== Statistiques descriptives de DOS ===\")\n",
    "print(y.describe())\n",
    "\n",
    "# 5. Histogramme de la variable continue DOS\n",
    "plt.figure(figsize=(6,4))\n",
    "y.hist(bins=20)\n",
    "plt.title(\"Distribution de la variable cible (DOS)\")\n",
    "plt.xlabel(\"DOS\")\n",
    "plt.ylabel(\"Effectif\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f769b",
   "metadata": {},
   "source": [
    "## 2. D√©finir le pr√©processing\n",
    "\n",
    "Pipeline de pr√©processing : imputation, standardisation, filtrage faible variance, filtrage low info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba3768d",
   "metadata": {},
   "source": [
    "## Configuration des pipelines STABL avec mod√®les d‚Äôarbres et lin√©aires\n",
    "\n",
    "Dans cette section, nous allons configurer plusieurs pipelines de s√©lection de variables et de mod√©lisation :\n",
    "- **STABL + Random Forest** : s√©lection de variables avec STABL utilisant Random Forest comme estimateur, avec recherche de grille (grid search) pour optimiser les hyperparam√®tres, et validation crois√©e (cross-validation).\n",
    "- **STABL + XGBoost** : m√™me principe, mais avec XGBoost comme estimateur.\n",
    "- **STABL + Lasso** : pipeline lin√©aire utilisant Lasso.\n",
    "- **STABL + ElasticNet** : pipeline lin√©aire utilisant ElasticNet.\n",
    "\n",
    "Pour chaque pipeline, nous appliquerons la s√©lection de variables, puis nous √©valuerons les performances en cross-validation et le nombre de variables s√©lectionn√©es. Les r√©sultats seront compar√©s √† la fin du notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aff8ec",
   "metadata": {},
   "source": [
    "## Cross-validation, sauvegarde des r√©sultats et visualisation\n",
    "\n",
    "Pour chaque mod√®le, nous allons :\n",
    "- Effectuer une cross-validation (stratifi√©e) sur le dataset.\n",
    "- Sauvegarder les courbes ROC et PR dans le dossier `Benchmarks results/` avec le nom du mod√®le et du dataset.\n",
    "- Sauvegarder les importances des variables s√©lectionn√©es et leur nombre.\n",
    "- Comparer les performances (AUC, accuracy, etc.) et le nombre de variables s√©lectionn√©es entre tous les mod√®les.\n",
    "\n",
    "Les r√©sultats seront visualis√©s sous forme de tableaux et de graphes pour faciliter la comparaison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ac694",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook permet de comparer la s√©lection de variables et les performances de classification entre STABL (Random Forest, XGBoost, Lasso, ElasticNet) sur le dataset CyTOF. Les courbes ROC, PR, importances des variables et tableaux de r√©sultats sont sauvegard√©s dans le dossier `Benchmarks results/` avec des noms explicites pour chaque mod√®le et dataset. Vous pouvez adapter ce pipeline √† d'autres datasets en modifiant la section de chargement des donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a935c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de pr√©processing (√† utiliser dans chaque pipeline)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from stabl.preprocessing import LowInfoFilter\n",
    "\n",
    "preprocessing = Pipeline([\n",
    "    (\"variance_threshold\", VarianceThreshold(threshold=0)),\n",
    "    (\"low_info_filter\", LowInfoFilter(max_nan_fraction=0.2)),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59710d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from stabl.data import load_onset_data\n",
    "from stabl.stacked_generalization import stacked_multi_omic\n",
    "\n",
    "# -------------------------\n",
    "# 0. Global config\n",
    "# -------------------------\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load data\n",
    "# -------------------------\n",
    "print(\"üì• Loading onset data ‚Ä¶\")\n",
    "X_dict, _, y, _, _, _ = load_onset_data(features_path, outcome_path)\n",
    "print(f\"‚úî Loaded {len(X_dict)} omics, {y.shape[0]} samples, outcome: {y.name}\")\n",
    "\n",
    "outer_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Pre-processing pipeline\n",
    "# -------------------------\n",
    "prepro = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"std\",     StandardScaler()),\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# 3. Hyper-parameter space\n",
    "# -------------------------\n",
    "param_dist = {\n",
    "    \"xgb__learning_rate\":    [0.03, 0.05, 0.07, 0.1],\n",
    "    \"xgb__n_estimators\":     [300, 500, 800],\n",
    "    \"xgb__max_depth\":        [2, 3, 4],\n",
    "    \"xgb__min_child_weight\": [1, 5, 10],\n",
    "    \"xgb__gamma\":            [0, 0.1, 0.3],\n",
    "    \"xgb__subsample\":        [0.6, 0.8, 1.0],\n",
    "    \"xgb__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"xgb__reg_alpha\":        [0, 0.5, 1, 2],\n",
    "    \"xgb__reg_lambda\":       [0.5, 1, 2, 4],\n",
    "}\n",
    "\n",
    "inner_cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def make_xgb_pipeline():\n",
    "    return Pipeline([\n",
    "        (\"prepro\", prepro),\n",
    "        (\"xgb\", XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            tree_method=\"hist\",\n",
    "            eval_metric=\"rmse\",\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "# -------------------------\n",
    "# 4. Per-omic tuning + OOF preds\n",
    "# -------------------------\n",
    "print(\"\\nüîß Hyper‚Äëparameter optimisation par omique ‚Ä¶\")\n",
    "\n",
    "preds_train = pd.DataFrame(index=y.index)\n",
    "best_params_by_omic = {}\n",
    "\n",
    "for omic_name, X_omic in tqdm(X_dict.items(), desc=\"Omics\", unit=\"omic\"):\n",
    "\n",
    "    pipe = make_xgb_pipeline()\n",
    "    rand_xgb = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=120,\n",
    "        scoring=\"r2\",\n",
    "        cv=inner_cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        random_state=RANDOM_STATE,\n",
    "        refit=True,\n",
    "    )\n",
    "\n",
    "    rand_xgb.fit(X_omic, y)  # pas d'early stopping -> √©vite l'erreur fit_params\n",
    "\n",
    "    best_params_by_omic[omic_name] = rand_xgb.best_params_\n",
    "    print(f\"‚úì {omic_name}: best CV R¬≤ = {rand_xgb.best_score_:.3f}\")\n",
    "\n",
    "    # OOF predictions with outer CV\n",
    "    fold_preds = pd.Series(index=y.index, dtype=float)\n",
    "    oof_bar = trange(outer_cv.get_n_splits(), desc=f\"OOF {omic_name}\", leave=False)\n",
    "    for k, (tr, te) in enumerate(outer_cv.split(X_omic, y)):\n",
    "        X_tr, X_te = X_omic.iloc[tr], X_omic.iloc[te]\n",
    "        y_tr       = y.iloc[tr]\n",
    "\n",
    "        best_pipe = make_xgb_pipeline().set_params(**rand_xgb.best_params_)\n",
    "        best_pipe.fit(X_tr, y_tr)\n",
    "        fold_preds.iloc[te] = best_pipe.predict(X_te)\n",
    "        oof_bar.update(1)\n",
    "    oof_bar.close()\n",
    "    preds_train[omic_name] = fold_preds\n",
    "\n",
    "# -------------------------\n",
    "# 5. Late-fusion (stacking)\n",
    "# -------------------------\n",
    "print(\"\\nüîó Empilement late‚Äëfusion ‚Ä¶\")\n",
    "stacked_df, weights = stacked_multi_omic(preds_train, y, task_type=\"regression\", n_iter=10000)\n",
    "print(\"‚úì R¬≤ stacked = {:.3f}\".format(r2_score(y, stacked_df[\"Stacked Gen. Predictions\"])))\n",
    "print(\"Poids des omiques :\\n\", weights)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Save artefacts\n",
    "# -------------------------\n",
    "output_dir = Path(\"results_multiomic\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üíæ Saving artefacts ‚Ä¶\")\n",
    "\n",
    "pd.concat({k: pd.Series(v) for k, v in best_params_by_omic.items()}, axis=1).to_csv(output_dir / \"best_params_by_omic.csv\")\n",
    "preds_train.to_csv(output_dir / \"preds_per_omic.csv\")\n",
    "stacked_df.to_csv(output_dir / \"stacked_predictions.csv\")\n",
    "weights.to_csv(output_dir / \"stacked_weights.csv\")\n",
    "\n",
    "print(\"\\nüéâ Pipeline termin√©. R√©sultats enregistr√©s dans\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9beea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor          # ‚Üê RF\n",
    "from stabl.data import load_onset_data\n",
    "from stabl.stacked_generalization import stacked_multi_omic\n",
    "\n",
    "# ------------------------- 0. Global config\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ------------------------- 1. Load data\n",
    "print(\"üì• Loading onset data ‚Ä¶\")\n",
    "X_dict, _, y, _, _, _ = load_onset_data(features_path, outcome_path)\n",
    "print(f\"‚úî Loaded {len(X_dict)} omics, {y.shape[0]} samples, outcome: {y.name}\")\n",
    "\n",
    "outer_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE)\n",
    "\n",
    "# ------------------------- 2. Pre-processing pipeline\n",
    "prepro = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"std\",     StandardScaler()),\n",
    "])\n",
    "\n",
    "# ------------------------- 3. Hyper-parameter space (RF)\n",
    "param_dist = {\n",
    "    \"rf__n_estimators\"     : [300, 500, 800, 1200],\n",
    "    \"rf__max_depth\"        : [None, 5, 10, 20],\n",
    "    \"rf__max_features\"     : [0.2, 0.4, 0.6, \"auto\"],\n",
    "    \"rf__min_samples_split\": [2, 5, 10],\n",
    "    \"rf__min_samples_leaf\" : [1, 2, 4],\n",
    "    \"rf__bootstrap\"        : [True, False],\n",
    "}\n",
    "\n",
    "inner_cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=RANDOM_STATE)\n",
    "\n",
    "def make_rf_pipeline():\n",
    "    return Pipeline([\n",
    "        (\"prepro\", prepro),\n",
    "        (\"rf\", RandomForestRegressor(\n",
    "            random_state   = RANDOM_STATE,\n",
    "            n_jobs         = -1,        # parall√©lisme interne\n",
    "            oob_score      = False,\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "# ------------------------- 4. Per-omic tuning + OOF preds\n",
    "print(\"\\nüîß Hyper-parameter optimisation RF par omique ‚Ä¶\")\n",
    "\n",
    "preds_train = pd.DataFrame(index=y.index)\n",
    "best_params_by_omic = {}\n",
    "\n",
    "for omic_name, X_omic in tqdm(X_dict.items(), desc=\"Omics\", unit=\"omic\"):\n",
    "\n",
    "    pipe = make_rf_pipeline()\n",
    "    rand_rf = RandomizedSearchCV(\n",
    "        estimator              = pipe,\n",
    "        param_distributions    = param_dist,\n",
    "        n_iter                 = 120,\n",
    "        scoring                = \"r2\",\n",
    "        cv                     = inner_cv,\n",
    "        n_jobs                 = -1,\n",
    "        verbose                = 0,\n",
    "        random_state           = RANDOM_STATE,\n",
    "        refit                  = True,\n",
    "    )\n",
    "\n",
    "    rand_rf.fit(X_omic, y)\n",
    "\n",
    "    best_params_by_omic[omic_name] = rand_rf.best_params_\n",
    "    print(f\"‚úì {omic_name}: best CV R¬≤ = {rand_rf.best_score_:.3f}\")\n",
    "\n",
    "    # OOF predictions with outer CV\n",
    "    fold_preds = pd.Series(index=y.index, dtype=float)\n",
    "    oof_bar = trange(outer_cv.get_n_splits(), desc=f\"OOF {omic_name}\", leave=False)\n",
    "    for k, (tr, te) in enumerate(outer_cv.split(X_omic, y)):\n",
    "        X_tr, X_te = X_omic.iloc[tr], X_omic.iloc[te]\n",
    "        y_tr       = y.iloc[tr]\n",
    "\n",
    "        best_pipe = make_rf_pipeline().set_params(**rand_rf.best_params_)\n",
    "        best_pipe.fit(X_tr, y_tr)\n",
    "        fold_preds.iloc[te] = best_pipe.predict(X_te)\n",
    "        oof_bar.update(1)\n",
    "    oof_bar.close()\n",
    "    preds_train[omic_name] = fold_preds\n",
    "\n",
    "# ------------------------- 5. Late-fusion (stacking)\n",
    "print(\"\\nüîó Empilement late-fusion ‚Ä¶\")\n",
    "stacked_df, weights = stacked_multi_omic(preds_train, y, task_type=\"regression\", n_iter=10000)\n",
    "print(\"‚úì R¬≤ stacked = {:.3f}\".format(r2_score(y, stacked_df[\"Stacked Gen. Predictions\"])))\n",
    "print(\"Poids des omiques :\\n\", weights)\n",
    "\n",
    "# ------------------------- 6. Save artefacts\n",
    "output_dir = Path(\"results_multiomic_RF\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üíæ Saving artefacts ‚Ä¶\")\n",
    "pd.concat({k: pd.Series(v) for k, v in best_params_by_omic.items()}, axis=1).to_csv(output_dir / \"best_params_by_omic.csv\")\n",
    "preds_train.to_csv(output_dir / \"preds_per_omic.csv\")\n",
    "stacked_df.to_csv(output_dir / \"stacked_predictions.csv\")\n",
    "weights.to_csv(output_dir / \"stacked_weights.csv\")\n",
    "\n",
    "print(\"\\nüéâ Pipeline termin√©. R√©sultats enregistr√©s dans\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from lightgbm import LGBMRegressor                      # ‚Üê LightGBM\n",
    "from stabl.data import load_onset_data\n",
    "from stabl.stacked_generalization import stacked_multi_omic\n",
    "\n",
    "# ------------------------- 0. Global config\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ------------------------- 1. Load data\n",
    "print(\"üì• Loading onset data ‚Ä¶\")\n",
    "X_dict, _, y, _, _, _ = load_onset_data(features_path, outcome_path)\n",
    "print(f\"‚úî Loaded {len(X_dict)} omics, {y.shape[0]} samples, outcome: {y.name}\")\n",
    "\n",
    "outer_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE)\n",
    "\n",
    "# ------------------------- 2. Pre-processing pipeline\n",
    "prepro = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"std\",     StandardScaler()),\n",
    "])\n",
    "\n",
    "# ------------------------- 3. Hyper-parameter space (LightGBM)\n",
    "param_dist = {\n",
    "    \"lgb__learning_rate\" : [0.01, 0.03, 0.05, 0.07],\n",
    "    \"lgb__n_estimators\"  : [500, 800, 1200],\n",
    "    \"lgb__max_depth\"     : [-1, 4, 6, 8],\n",
    "    \"lgb__num_leaves\"    : [31, 63, 127, 255],       # doit rester ‚â§ 2**max_depth\n",
    "    \"lgb__subsample\"     : [0.6, 0.8, 1.0],\n",
    "    \"lgb__colsample_bytree\":[0.6, 0.8, 1.0],\n",
    "    \"lgb__min_child_samples\":[5, 10, 20],\n",
    "    \"lgb__reg_alpha\"     : [0, 0.5, 1],\n",
    "    \"lgb__reg_lambda\"    : [0, 0.5, 1],\n",
    "}\n",
    "\n",
    "inner_cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=RANDOM_STATE)\n",
    "\n",
    "def make_lgb_pipeline():\n",
    "    return Pipeline([\n",
    "        (\"prepro\", prepro),\n",
    "        (\"lgb\", LGBMRegressor(\n",
    "            objective     = \"regression\",\n",
    "            random_state  = RANDOM_STATE,\n",
    "            boosting_type = \"gbdt\",\n",
    "            n_jobs        = -1,\n",
    "            verbose       = -1,\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "# ------------------------- 4. Per-omic tuning + OOF preds\n",
    "print(\"\\nüîß Hyper-parameter optimisation LightGBM par omique ‚Ä¶\")\n",
    "\n",
    "preds_train = pd.DataFrame(index=y.index)\n",
    "best_params_by_omic = {}\n",
    "\n",
    "for omic_name, X_omic in tqdm(X_dict.items(), desc=\"Omics\", unit=\"omic\"):\n",
    "\n",
    "    pipe = make_lgb_pipeline()\n",
    "    rand_lgb = RandomizedSearchCV(\n",
    "        estimator           = pipe,\n",
    "        param_distributions = param_dist,\n",
    "        n_iter              = 120,\n",
    "        scoring             = \"r2\",\n",
    "        cv                  = inner_cv,\n",
    "        n_jobs              = -1,\n",
    "        random_state        = RANDOM_STATE,\n",
    "        verbose             = 0,\n",
    "        refit               = True,\n",
    "    )\n",
    "\n",
    "    rand_lgb.fit(X_omic, y)\n",
    "\n",
    "    best_params_by_omic[omic_name] = rand_lgb.best_params_\n",
    "    print(f\"‚úì {omic_name}: best CV R¬≤ = {rand_lgb.best_score_:.3f}\")\n",
    "\n",
    "    # OOF predictions with outer CV\n",
    "    fold_preds = pd.Series(index=y.index, dtype=float)\n",
    "    oof_bar = trange(outer_cv.get_n_splits(), desc=f\"OOF {omic_name}\", leave=False)\n",
    "    for k, (tr, te) in enumerate(outer_cv.split(X_omic, y)):\n",
    "        X_tr, X_te = X_omic.iloc[tr], X_omic.iloc[te]\n",
    "        y_tr       = y.iloc[tr]\n",
    "\n",
    "        best_pipe = make_lgb_pipeline().set_params(**rand_lgb.best_params_)\n",
    "        best_pipe.fit(X_tr, y_tr)\n",
    "        fold_preds.iloc[te] = best_pipe.predict(X_te)\n",
    "        oof_bar.update(1)\n",
    "    oof_bar.close()\n",
    "    preds_train[omic_name] = fold_preds\n",
    "\n",
    "# ------------------------- 5. Late-fusion (stacking)\n",
    "print(\"\\nüîó Empilement late-fusion ‚Ä¶\")\n",
    "stacked_df, weights = stacked_multi_omic(preds_train, y, task_type=\"regression\", n_iter=10000)\n",
    "print(\"‚úì R¬≤ stacked = {:.3f}\".format(r2_score(y, stacked_df[\"Stacked Gen. Predictions\"])))\n",
    "print(\"Poids des omiques :\\n\", weights)\n",
    "\n",
    "# ------------------------- 6. Save artefacts\n",
    "output_dir = Path(\"results_multiomic_LightGBM\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üíæ Saving artefacts ‚Ä¶\")\n",
    "pd.concat({k: pd.Series(v) for k, v in best_params_by_omic.items()}, axis=1).to_csv(output_dir / \"best_params_by_omic.csv\")\n",
    "preds_train.to_csv(output_dir / \"preds_per_omic.csv\")\n",
    "stacked_df.to_csv(output_dir / \"stacked_predictions.csv\")\n",
    "weights.to_csv(output_dir / \"stacked_weights.csv\")\n",
    "\n",
    "print(\"\\nüéâ Pipeline termin√©. R√©sultats enregistr√©s dans\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace3092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: filename 'x_linear' inconnu, on prend tous les stims standards.\n",
      "Warning: pas de colonnes pour le stim 'Unstim'.\n",
      "Warning: pas de colonnes pour le stim 'TNFa'.\n",
      "Warning: pas de colonnes pour le stim 'LPS'.\n",
      "Warning: pas de colonnes pour le stim 'IL246'.\n",
      "Warning: pas de colonnes pour le stim 'IFNa'.\n",
      "Warning: pas de colonnes pour le stim 'GMCSF'.\n",
      "Warning: pas de colonnes pour le stim 'PI'.\n",
      "Warning: pas de colonnes pour le stim 'IL33'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Aucun stim d√©tect√©: v√©rifie tes noms de colonnes/features.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 169\u001b[39m\n\u001b[32m    156\u001b[39m estimators[\u001b[33m\"\u001b[39m\u001b[33mstabl_lgb\u001b[39m\u001b[33m\"\u001b[39m]  = estimators[\u001b[33m\"\u001b[39m\u001b[33mstabl_lasso\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# placeholder vide\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# final_classifiers = {\"Logit\": LogisticRegression(penalty=\"l1\", solver=\"liblinear\", class_weight=\"balanced\", max_iter=int(1e6), random_state=42),\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m#                      \"RandomForest\": RandomForestClassifier(n_estimators=500, random_state=42),\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m#                      \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", n_estimators=200,random_state=42),\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# X_train, X_valid, y_train, y_valid, ids, task_type = data.load_covid_19(\"data/COVID-19\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m X_train, X_val, y_train, y_val, groups, task_type = \u001b[43mload_onset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# 7. Lancement du benchmark multi-omic STABL\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    174\u001b[39m save_path = \u001b[33m\"\u001b[39m\u001b[33m./Benchmarks results/Regresssion data Olivier + XGB+ 200 bootstraps/KO\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Stanford/venv/lib/python3.11/site-packages/stabl/data.py:195\u001b[39m, in \u001b[36mload_onset_data\u001b[39m\u001b[34m(features_path, outcome_path)\u001b[39m\n\u001b[32m    193\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: pas de colonnes pour le stim \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m X_train:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAucun stim d√©tect√©: v√©rifie tes noms de colonnes/features.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# 5. Pas de jeu de validation s√©par√© dans cette fonction\u001b[39;00m\n\u001b[32m    198\u001b[39m X_val = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Aucun stim d√©tect√©: v√©rifie tes noms de colonnes/features."
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Imports et configuration g√©n√©rale\n",
    "# ===============================\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stabl import data\n",
    "from stabl.multi_omic_pipelines import multi_omic_stabl_cv\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, RepeatedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from stabl.stabl import Stabl\n",
    "from stabl.adaptive import ALogitLasso\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from stabl.data import load_onset_data\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===============================\n",
    "# 2. D√©finition des splits de validation crois√©e\n",
    "# ===============================\n",
    "# Outer CV pour l'√©valuation globale, inner CV pour la recherche d'hyperparam√®tres\n",
    "outer_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "inner_cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 3. D√©finition des estimateurs et grilles d'hyperparam√®tres\n",
    "# ===============================\n",
    "artificial_type = \"knockoff\"  # ou \"random_permutation\"\n",
    "\n",
    "# Lasso\n",
    "lasso = Lasso(max_iter=int(1e6), random_state=42)\n",
    "lasso_cv = GridSearchCV(lasso, param_grid={\"alpha\": np.logspace(-2, 2, 30)}, scoring=\"r2\", cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "#ElasticNet\n",
    "en = ElasticNet(max_iter=int(1e6), random_state=42)\n",
    "en_params = {\"alpha\": np.logspace(-2, 2, 10), \"l1_ratio\": [0.5, 0.7, 0.9]}\n",
    "en_cv = GridSearchCV(en, param_grid=en_params, scoring=\"r2\", cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# RandomForest\n",
    "rf = RandomForestRegressor(random_state=42, max_features=0.2)\n",
    "rf_grid = {\"max_depth\": [3, 5, 7, 9, 11]}\n",
    "rf_cv = GridSearchCV(rf, scoring='r2', param_grid=rf_grid, cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBRegressor(random_state=42, importance_type=\"gain\", objective=\"reg:squarederror\")\n",
    "xgb_grid = {\"max_depth\": [3, 6, 9], \"reg_alpha\": [0, 0.5, 1, 2]}\n",
    "xgb_cv = GridSearchCV(xgb, scoring='r2', param_grid=xgb_grid, cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# CatBoost\n",
    "cb = CatBoostRegressor(random_state=42)\n",
    "cb_grid = {\"depth\": [3, 5, 7], \"learning_rate\": [0.01, 0.1, 0.2], \"l2_leaf_reg\": [1, 3, 5]}\n",
    "cb_cv = GridSearchCV(cb, scoring='r2', param_grid=cb_grid, cv=inner_cv, n_jobs=-1, verbose=0)\n",
    "\n",
    "# LightGBM\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "lgb_grid = {\"max_depth\": [4, 6, 8], \"learning_rate\": [0.01, 0.1], \"num_leaves\": [31, 63, 127], \"reg_alpha\": [0, 1], \"reg_lambda\": [0, 1]}\n",
    "lgb_cv = GridSearchCV(estimator=lgb, param_grid=lgb_grid, scoring=\"r2\", cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# ===============================\n",
    "# 4. D√©finition des estimateurs STABL\n",
    "# ===============================\n",
    "stabl_lasso = Stabl(\n",
    "    base_estimator=lasso,\n",
    "    n_bootstraps=200,\n",
    "    artificial_type=artificial_type,\n",
    "    artificial_proportion=1.,\n",
    "    replace=False,\n",
    "    fdr_threshold_range=np.arange(0.1, 1, 0.01),\n",
    "    sample_fraction=0.5,\n",
    "    random_state=42,\n",
    "    lambda_grid={\"alpha\": np.logspace(-2, 2, 10)},\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stabl_en = clone(stabl_lasso).set_params(\n",
    "    base_estimator=en,\n",
    "    n_bootstraps=100,\n",
    "    lambda_grid=[{\"C\": np.logspace(-2, 1, 5), \"l1_ratio\": [0.5, 0.9]}],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stabl_rf = clone(stabl_lasso).set_params(\n",
    "    base_estimator=rf,\n",
    "    n_bootstraps=200,\n",
    "    lambda_grid=rf_grid,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stabl_xgb = clone(stabl_lasso).set_params(\n",
    "    base_estimator=xgb,\n",
    "    n_bootstraps=200,\n",
    "    lambda_grid=[xgb_grid],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stabl_cb = clone(stabl_lasso).set_params(\n",
    "    base_estimator=cb,\n",
    "    n_bootstraps=200,\n",
    "    lambda_grid=[cb_grid],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stabl_lgb = clone(stabl_lasso).set_params(\n",
    "    base_estimator=lgb,\n",
    "    n_bootstraps=200,\n",
    "    lambda_grid=[lgb_grid],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 5. Dictionnaire des estimateurs pour le benchmark\n",
    "# ===============================\n",
    "estimators = {\n",
    "    \"lasso\": lasso_cv,\n",
    "    \"rf\": rf_cv,\n",
    "    \"xgb\": xgb_cv,\n",
    "#   \"cb\": cb_cv,\n",
    "#   \"lgb\": lgb_cv,\n",
    "    \"stabl_lasso\": stabl_lasso,\n",
    "    \"stabl_rf\": stabl_rf,\n",
    "    \"stabl_xgb\": stabl_xgb,\n",
    "#   \"stabl_cb\" : stabl_cb,\n",
    "#   \"stabl_lgb\": stabl_lgb,\n",
    "    }\n",
    "\n",
    "models = [\n",
    "    \"Lasso\",\n",
    "    \"RandomForest\",\n",
    "    \"XGBoost\",\n",
    "#    \"CatBoost\",\n",
    "#    \"LightGBM\",\n",
    "    \"STABL Lasso\",\n",
    "    \"STABL RandomForest\",\n",
    "    \"STABL XGBoost\",\n",
    "#    \"STABL CatBoost\"\n",
    "#    \"STABL LightGBM\"\n",
    "]\n",
    "\n",
    "\n",
    "# juste apr√®s avoir construit ton dict estimators, ajoute :\n",
    "estimators[\"en\"]        = estimators[\"lasso\"]        # placeholder vide\n",
    "estimators[\"stabl_en\"]  = estimators[\"stabl_lasso\"]  # placeholder vide\n",
    "\n",
    "estimators[\"cb\"]        = estimators[\"lasso\"]        # placeholder vide\n",
    "estimators[\"stabl_cb\"]  = estimators[\"stabl_lasso\"]  # placeholder vide\n",
    "\n",
    "estimators[\"lgb\"]        = estimators[\"lasso\"]        # placeholder vide\n",
    "estimators[\"stabl_lgb\"]  = estimators[\"stabl_lasso\"]  # placeholder vide\n",
    "\n",
    "\n",
    "# final_classifiers = {\"Logit\": LogisticRegression(penalty=\"l1\", solver=\"liblinear\", class_weight=\"balanced\", max_iter=int(1e6), random_state=42),\n",
    "#                      \"RandomForest\": RandomForestClassifier(n_estimators=500, random_state=42),\n",
    "#                      \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", n_estimators=200,random_state=42),\n",
    "#                     }\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 6. Chargement des donn√©es (exemple COVID-19)\n",
    "# ===============================\n",
    "# X_train, X_valid, y_train, y_valid, ids, task_type = data.load_covid_19(\"data/COVID-19\")\n",
    "X_train, X_val, y_train, y_val, groups, task_type = load_onset_data(features_path, outcome_path)\n",
    "\n",
    "# ===============================\n",
    "# 7. Lancement du benchmark multi-omic STABL\n",
    "# ===============================\n",
    "save_path = \"./Benchmarks results/Regresssion data Olivier + XGB+ 200 bootstraps/KO\"\n",
    "\n",
    "# Nettoyage du dossier de sauvegarde si besoin\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "\n",
    "print(\"Run CV on Olivier dataset\")\n",
    "# print(groups.value_counts())\n",
    "\n",
    "multi_omic_stabl_cv(\n",
    "    data_dict=X_train,\n",
    "    y=y_train,\n",
    "    outer_splitter=outer_cv,\n",
    "    estimators=estimators,\n",
    "    task_type=task_type,\n",
    "    save_path=save_path,\n",
    "    outer_groups=groups,\n",
    "    early_fusion=False,\n",
    "    late_fusion=True,\n",
    "    n_iter_lf=1000,\n",
    "    models=models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a46434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noeamar/Documents/Stanford/venv/lib/python3.11/site-packages/stabl/multi_omic_pipelines.py:3: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Le fichier y contient plus de lignes que X.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Stanford/venv/lib/python3.11/site-packages/stabl/data.py:237\u001b[39m, in \u001b[36mload_artificial_data\u001b[39m\u001b[34m(X_path, y_path)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_df.shape[\u001b[32m1\u001b[39m] == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[32m    238\u001b[39m y = y_df.iloc[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 171\u001b[39m\n\u001b[32m    158\u001b[39m estimators[\u001b[33m\"\u001b[39m\u001b[33mstabl_lgb\u001b[39m\u001b[33m\"\u001b[39m]  = estimators[\u001b[33m\"\u001b[39m\u001b[33mstabl_lasso\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# placeholder vide\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# final_classifiers = {\"Logit\": LogisticRegression(penalty=\"l1\", solver=\"liblinear\", class_weight=\"balanced\", max_iter=int(1e6), random_state=42),\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m#                      \"RandomForest\": RandomForestClassifier(n_estimators=500, random_state=42),\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m#                      \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", n_estimators=200,random_state=42),\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# X_train, X_valid, y_train, y_valid, ids, task_type = data.load_covid_19(\"data/COVID-19\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m X_train, X_val, y_train, y_val, groups, task_type = \u001b[43mload_artificial_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# 7. Lancement du benchmark multi-omic STABL\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    176\u001b[39m save_path = \u001b[33m\"\u001b[39m\u001b[33m./Benchmarks results/Regresssion Artificial data + XGB + 200 bootstraps/KO\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Stanford/venv/lib/python3.11/site-packages/stabl/data.py:249\u001b[39m, in \u001b[36mload_artificial_data\u001b[39m\u001b[34m(X_path, y_path)\u001b[39m\n\u001b[32m    247\u001b[39m         y.index = X.index[: \u001b[38;5;28mlen\u001b[39m(y)]\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLe fichier y contient plus de lignes que X.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# ---------- 3. alignement index commun -------------------------------\u001b[39;00m\n\u001b[32m    252\u001b[39m common_idx = X.index.intersection(y.index)\n",
      "\u001b[31mValueError\u001b[39m: Le fichier y contient plus de lignes que X."
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Imports et configuration g√©n√©rale\n",
    "# ===============================\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stabl import data\n",
    "from stabl.multi_omic_pipelines import multi_omic_stabl_cv\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, RepeatedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from stabl.stabl import Stabl\n",
    "from stabl.adaptive import ALogitLasso\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from stabl.data import load_onset_data, load_artificial_data\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===============================\n",
    "# 2. D√©finition des splits de validation crois√©e\n",
    "# ===============================\n",
    "# Outer CV pour l'√©valuation globale, inner CV pour la recherche d'hyperparam√®tres\n",
    "outer_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "inner_cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "features_path = \"/Users/noeamar/Documents/Stanford/datasets/X_linear.csv\"\n",
    "outcome_path = \"/Users/noeamar/Documents/Stanford/datasets/y_linear.csv\"\n",
    "# ===============================\n",
    "# 3. D√©finition des estimateurs et grilles d'hyperparam√®tres\n",
    "# ===============================\n",
    "artificial_type = \"knockoff\"  # ou \"random_permutation\"\n",
    "\n",
    "# Lasso\n",
    "lasso = Lasso(max_iter=int(1e6), random_state=42)\n",
    "lasso_cv = GridSearchCV(lasso, param_grid={\"alpha\": np.logspace(-2, 2, 30)}, scoring=\"r2\", cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "#ElasticNet\n",
    "en = ElasticNet(max_iter=int(1e6), random_state=42)\n",
    "en_params = {\"alpha\": np.logspace(-2, 2, 10), \"l1_ratio\": [0.5, 0.7, 0.9]}\n",
    "en_cv = GridSearchCV(en, param_grid=en_params, scoring=\"r2\", cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# RandomForest\n",
    "rf = RandomForestRegressor(random_state=42, max_features=0.2)\n",
    "rf_grid = {\"max_depth\": [3, 5, 7, 9, 11]}\n",
    "rf_cv = GridSearchCV(rf, scoring='r2', param_grid=rf_grid, cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBRegressor(random_state=42, importance_type=\"gain\", objective=\"reg:squarederror\")\n",
    "xgb_grid = {\"max_depth\": [3, 6, 9], \"reg_alpha\": [0, 0.5, 1, 2]}\n",
    "xgb_cv = GridSearchCV(xgb, scoring='r2', param_grid=xgb_grid, cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# CatBoost\n",
    "cb = CatBoostRegressor(random_state=42)\n",
    "cb_grid = {\"depth\": [3, 5, 7], \"learning_rate\": [0.01, 0.1, 0.2], \"l2_leaf_reg\": [1, 3, 5]}\n",
    "cb_cv = GridSearchCV(cb, scoring='r2', param_grid=cb_grid, cv=inner_cv, n_jobs=-1, verbose=0)\n",
    "\n",
    "# LightGBM\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "lgb_grid = {\"max_depth\": [4, 6, 8], \"learning_rate\": [0.01, 0.1], \"num_leaves\": [31, 63, 127], \"reg_alpha\": [0, 1], \"reg_lambda\": [0, 1]}\n",
    "lgb_cv = GridSearchCV(estimator=lgb, param_grid=lgb_grid, scoring=\"r2\", cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# ===============================\n",
    "# 4. D√©finition des estimateurs STABL\n",
    "# ===============================\n",
    "stabl_lasso = Stabl(\n",
    "    base_estimator=lasso,\n",
    "    n_bootstraps=300,\n",
    "    artificial_type=artificial_type,\n",
    "    artificial_proportion=1.,\n",
    "    replace=False,\n",
    "    fdr_threshold_range=np.arange(0.1, 1, 0.01),\n",
    "    sample_fraction=0.5,\n",
    "    random_state=42,\n",
    "    lambda_grid={\"alpha\": np.logspace(-2, 2, 10)},\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stabl_en = clone(stabl_lasso).set_params(\n",
    "    base_estimator=en,\n",
    "    n_bootstraps=100,\n",
    "    lambda_grid=[{\"C\": np.logspace(-2, 1, 5), \"l1_ratio\": [0.5, 0.9]}],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stabl_rf = clone(stabl_lasso).set_params(\n",
    "    base_estimator=rf,\n",
    "    n_bootstraps=300,\n",
    "    lambda_grid=rf_grid,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stabl_xgb = clone(stabl_lasso).set_params(\n",
    "    base_estimator=xgb,\n",
    "    n_bootstraps=300,\n",
    "    lambda_grid=[xgb_grid],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stabl_cb = clone(stabl_lasso).set_params(\n",
    "    base_estimator=cb,\n",
    "    n_bootstraps=300,\n",
    "    lambda_grid=[cb_grid],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stabl_lgb = clone(stabl_lasso).set_params(\n",
    "    base_estimator=lgb,\n",
    "    n_bootstraps=300,\n",
    "    lambda_grid=[lgb_grid],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 5. Dictionnaire des estimateurs pour le benchmark\n",
    "# ===============================\n",
    "estimators = {\n",
    "    \"lasso\": lasso_cv,\n",
    "    \"rf\": rf_cv,\n",
    "    \"xgb\": xgb_cv,\n",
    "#   \"cb\": cb_cv,\n",
    "#   \"lgb\": lgb_cv,\n",
    "    \"stabl_lasso\": stabl_lasso,\n",
    "    \"stabl_rf\": stabl_rf,\n",
    "    \"stabl_xgb\": stabl_xgb,\n",
    "#   \"stabl_cb\" : stabl_cb,\n",
    "#   \"stabl_lgb\": stabl_lgb,\n",
    "    }\n",
    "\n",
    "models = [\n",
    "    \"Lasso\",\n",
    "    \"RandomForest\",\n",
    "    \"XGBoost\",\n",
    "#    \"CatBoost\",\n",
    "#    \"LightGBM\",\n",
    "    \"STABL Lasso\",\n",
    "    \"STABL RandomForest\",\n",
    "    \"STABL XGBoost\",\n",
    "#    \"STABL CatBoost\"\n",
    "#    \"STABL LightGBM\"\n",
    "]\n",
    "\n",
    "\n",
    "# juste apr√®s avoir construit ton dict estimators, ajoute :\n",
    "estimators[\"en\"]        = estimators[\"lasso\"]        # placeholder vide\n",
    "estimators[\"stabl_en\"]  = estimators[\"stabl_lasso\"]  # placeholder vide\n",
    "\n",
    "estimators[\"cb\"]        = estimators[\"lasso\"]        # placeholder vide\n",
    "estimators[\"stabl_cb\"]  = estimators[\"stabl_lasso\"]  # placeholder vide\n",
    "\n",
    "estimators[\"lgb\"]        = estimators[\"lasso\"]        # placeholder vide\n",
    "estimators[\"stabl_lgb\"]  = estimators[\"stabl_lasso\"]  # placeholder vide\n",
    "\n",
    "\n",
    "# final_classifiers = {\"Logit\": LogisticRegression(penalty=\"l1\", solver=\"liblinear\", class_weight=\"balanced\", max_iter=int(1e6), random_state=42),\n",
    "#                      \"RandomForest\": RandomForestClassifier(n_estimators=500, random_state=42),\n",
    "#                      \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", n_estimators=200,random_state=42),\n",
    "#                     }\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 6. Chargement des donn√©es (exemple COVID-19)\n",
    "# ===============================\n",
    "# X_train, X_valid, y_train, y_valid, ids, task_type = data.load_covid_19(\"data/COVID-19\")\n",
    "X_train, X_val, y_train, y_val, groups, task_type = load_artificial_data(features_path, outcome_path)\n",
    "\n",
    "# ===============================\n",
    "# 7. Lancement du benchmark multi-omic STABL\n",
    "# ===============================\n",
    "save_path = \"./Benchmarks results/Regresssion Artificial data + XGB + 200 bootstraps/KO\"\n",
    "\n",
    "# Nettoyage du dossier de sauvegarde si besoin\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "\n",
    "print(\"Run CV on Olivier dataset\")\n",
    "# print(groups.value_counts())\n",
    "\n",
    "multi_omic_stabl_cv(\n",
    "    data_dict=X_train,\n",
    "    y=y_train,\n",
    "    outer_splitter=outer_cv,\n",
    "    estimators=estimators,\n",
    "    task_type=task_type,\n",
    "    save_path=save_path,\n",
    "    outer_groups=groups,\n",
    "    early_fusion=False,\n",
    "    late_fusion=True,\n",
    "    n_iter_lf=1000,\n",
    "    models=models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Imports et configuration g√©n√©rale\n",
    "# ===============================\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stabl import data\n",
    "from stabl.multi_omic_pipelines import multi_omic_stabl_cv_combined  # nouvelle fonction\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from stabl.stabl import Stabl\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from stabl.data import load_onset_data\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "features_path = \"/Users/noeamar/Documents/Stanford/data/olivier_data/ina_13OG_df_168_filtered_allstim_new.csv\"\n",
    "outcome_path = \"/Users/noeamar/Documents/Stanford/data/olivier_data/outcome_table_all_pre.csv\"\n",
    "\n",
    "# ===============================\n",
    "# 2. D√©finition des splits de validation crois√©e\n",
    "# ===============================\n",
    "outer_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "inner_cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 3. D√©finition des estimateurs et grilles d'hyperparam√®tres\n",
    "# ===============================\n",
    "artificial_type = \"knockoff\"\n",
    "# base estimators\n",
    "lasso = Lasso(max_iter=int(1e6), random_state=42)\n",
    "lasso_cv = GridSearchCV(lasso, {'alpha': np.logspace(-2,2,30)}, scoring='r2', cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "en = ElasticNet(max_iter=int(1e6), random_state=42)\n",
    "en_cv = GridSearchCV(en, {'alpha': np.logspace(-2,2,10), 'l1_ratio':[0.5,0.7,0.9]}, scoring='r2', cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, max_features=0.2)\n",
    "rf_cv = GridSearchCV(rf, {'max_depth':[3,5,7,9,11]}, scoring='r2', cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "xgb = XGBRegressor(random_state=42, importance_type='gain', objective='reg:squarederror')\n",
    "xgb_cv = GridSearchCV(xgb, {'max_depth':[3,6,9], 'reg_alpha':[0,0.5,1,2]}, scoring='r2', cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "cb = CatBoostRegressor(random_state=42)\n",
    "cb_cv = GridSearchCV(cb, {'depth':[3,5,7], 'learning_rate':[0.01,0.1,0.2], 'l2_leaf_reg':[1,3,5]}, scoring='r2', cv=inner_cv, n_jobs=-1, verbose=0)\n",
    "\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "lgb_cv = GridSearchCV(lgb, {'max_depth':[4,6,8], 'learning_rate':[0.01,0.1], 'num_leaves':[31,63,127], 'reg_alpha':[0,1], 'reg_lambda':[0,1]}, scoring='r2', cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# ===============================\n",
    "# 4. D√©finition des estimateurs STABL\n",
    "# ===============================\n",
    "stabl_lasso = Stabl(base_estimator=lasso, n_bootstraps=100, artificial_type=artificial_type,\n",
    "                    artificial_proportion=1., replace=False,\n",
    "                    fdr_threshold_range=np.arange(0.1,1,0.01), sample_fraction=0.5,\n",
    "                    random_state=42, lambda_grid={'alpha':np.logspace(-2,2,10)}, verbose=1)\n",
    "stabl_rf = clone(stabl_lasso).set_params(base_estimator=rf, lambda_grid={'max_depth':[3,5,7,9,11]})\n",
    "stabl_xgb = clone(stabl_lasso).set_params(base_estimator=xgb, lambda_grid=[{'max_depth':[3,6,9], 'reg_alpha':[0,0.5,1,2]}])\n",
    "stabl_cb  = clone(stabl_lasso).set_params(base_estimator=cb, lambda_grid=[{'depth':[3,5,7], 'learning_rate':[0.01,0.1,0.2], 'l2_leaf_reg':[1,3,5]}])\n",
    "stabl_lgb = clone(stabl_lasso).set_params(base_estimator=lgb, lambda_grid=[{'max_depth':[4,6,8], 'learning_rate':[0.01,0.1], 'num_leaves':[31,63,127], 'reg_alpha':[0,1], 'reg_lambda':[0,1]}])\n",
    "\n",
    "# ===============================\n",
    "# 5. Dictionnaire des estimateurs pour le benchmark\n",
    "# ===============================\n",
    "estimators = {\n",
    "#    'lasso': lasso_cv,\n",
    "#    'rf':   rf_cv,\n",
    "    'xgb':  xgb_cv,\n",
    "#    'cb':   cb_cv,\n",
    "#    'lgb':  lgb_cv,\n",
    "#    'stabl_lasso': stabl_lasso,\n",
    "#    'stabl_rf':    stabl_rf,\n",
    "    'stabl_xgb':   stabl_xgb,\n",
    "#    'stabl_cb':    stabl_cb,\n",
    "#    'stabl_lgb':   stabl_lgb\n",
    "}\n",
    "models = [\n",
    "#    'Lasso',\n",
    "#    'RandomForest',\n",
    "    'XGBoost',\n",
    "#    'CatBoost',\n",
    "#    'LightGBM',\n",
    "#    'STABL Lasso',\n",
    "#    'STABL RandomForest',\n",
    "    'STABL XGBoost',\n",
    "#    'STABL CatBoost',\n",
    "#    'STABL LightGBM',\n",
    "    'STABL_Combined'\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# 6. Chargement des donn√©es\n",
    "# ===============================\n",
    "X_train, X_val, y_train, y_val, groups, task_type = load_onset_data(features_path, outcome_path)\n",
    "\n",
    "# ===============================\n",
    "# 7. Lancement du benchmark multi-omic STABL combin√©\n",
    "# ===============================\n",
    "save_path = './Benchmarks results/Combined Selection/KO'\n",
    "if os.path.exists(save_path): shutil.rmtree(save_path)\n",
    "\n",
    "# juste apr√®s avoir construit ton dict estimators, ajoute :\n",
    "estimators[\"en\"]        = estimators[\"xgb\"]        # placeholder vide\n",
    "estimators[\"stabl_en\"]  = estimators[\"stabl_xgb\"]  # placeholder vide\n",
    "\n",
    "estimators[\"cb\"]        = estimators[\"xgb\"]        # placeholder vide\n",
    "estimators[\"stabl_cb\"]  = estimators[\"stabl_xgb\"]  # placeholder vide\n",
    "\n",
    "estimators[\"lgb\"]        = estimators[\"xgb\"]        # placeholder vide\n",
    "estimators[\"stabl_lgb\"]  = estimators[\"stabl_xgb\"]  # placeholder vide\n",
    "\n",
    "estimators[\"rf\"]        = estimators[\"xgb\"]        # placeholder vide\n",
    "estimators[\"stabl_rf\"]  = estimators[\"stabl_xgb\"]  # placeholder vide\n",
    "\n",
    "estimators[\"lasso\"]        = estimators[\"xgb\"]        # placeholder vide\n",
    "estimators[\"stabl_lasso\"]  = estimators[\"stabl_xgb\"]  # placeholder vide\n",
    "\n",
    "multi_omic_stabl_cv_combined(\n",
    "    data_dict     = X_train,\n",
    "    y             = y_train,\n",
    "    outer_splitter= outer_cv,\n",
    "    estimators    = estimators,\n",
    "    task_type     = task_type,\n",
    "    save_path     = save_path,\n",
    "    outer_groups  = groups,\n",
    "    early_fusion  = False,\n",
    "    late_fusion   = False,\n",
    "    n_iter_lf     = 1000,\n",
    "    vote_weights  = [0.4, 0.3, 0.3], # Poids pour Lasso, RF, XGB\n",
    "    fdr_alpha     = 0.3,\n",
    "    models        = models\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06bce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0.¬†Imports,¬†config¬†g√©n√©rale¬†&¬†parall√©lisme\n",
    "# ===============================\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Limite explicite¬†: **un seul thread BLAS/OMP** par processus\n",
    "# (√©vite un double parall√©lisme si Joblib ouvre d√©j√† plusieurs processus)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# STABL & data utils\n",
    "from stabl.multi_omic_pipelines import multi_omic_stabl_cv\n",
    "from stabl.stabl import Stabl\n",
    "from stabl.adaptive import ALogitLasso\n",
    "from stabl.data import load_onset_data\n",
    "\n",
    "# Scikit‚Äëlearn models & helpers\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Gradient‚Äëboosting libs\n",
    "from xgboost import XGBRegressor\n",
    "# from catboost import CatBoostRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===============================\n",
    "# 1.¬†Cross‚Äëvalidation splitters\n",
    "# ===============================\n",
    "outer_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "inner_cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 2.¬†Estimators & hyper‚Äëparameter grids\n",
    "# ===============================\n",
    "artificial_type = \"knockoff\"  # ou \"random_permutation\"\n",
    "\n",
    "# -- Lasso\n",
    "lasso      = Lasso(max_iter=int(1e6), random_state=42)\n",
    "lasso_grid = {\"alpha\": np.logspace(-2, 2, 30)}\n",
    "lasso_cv   = GridSearchCV(lasso, param_grid=lasso_grid, scoring=\"r2\", cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# -- ElasticNet\n",
    "en        = ElasticNet(max_iter=int(1e6), random_state=42)\n",
    "en_grid   = {\"alpha\": np.logspace(-2, 2, 10), \"l1_ratio\": [0.5, 0.7, 0.9]}\n",
    "en_cv     = GridSearchCV(en, param_grid=en_grid, scoring=\"r2\", cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# -- RandomForest | *un seul thread par processus Joblib*\n",
    "rf      = RandomForestRegressor(random_state=42, max_features=0.2, n_jobs=1)\n",
    "rf_grid = {\"max_depth\": [3, 5, 7, 9, 11]}\n",
    "rf_cv   = GridSearchCV(rf, param_grid=rf_grid, scoring=\"r2\", cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# -- XGBoost | *un seul thread interne pour √©viter le double parall√©lisme*\n",
    "xgb      = XGBRegressor(\n",
    "    random_state=42,\n",
    "    objective=\"reg:squarederror\",\n",
    "    importance_type=\"gain\",\n",
    "    tree_method=\"hist\",\n",
    "    nthread=1           # <‚Äë‚Äë¬†cl√© !\n",
    ")\n",
    "xgb_grid = {\"max_depth\": [3, 6, 9], \"reg_alpha\": [0, 0.5, 1, 2]}\n",
    "xgb_cv   = GridSearchCV(xgb, param_grid=xgb_grid, scoring=\"r2\", cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# (CatBoost / LightGBM d√©sactiv√©s pour l'instant)\n",
    "\n",
    "# ===============================\n",
    "# 3.¬†STABL wrappers (n_jobs=1 pour √©viter un 3·µâ niveau de parall√©lisme)\n",
    "# ===============================\n",
    "stabl_base_kwargs = dict(\n",
    "    n_bootstraps=100,\n",
    "    artificial_type=artificial_type,\n",
    "    artificial_proportion=1.0,\n",
    "    replace=False,\n",
    "    fdr_threshold_range=np.arange(0.1, 1, 0.01),\n",
    "    sample_fraction=0.5,\n",
    "    random_state=42,\n",
    "    verbose=1, \n",
    ")\n",
    "\n",
    "stabl_lasso = Stabl(base_estimator=lasso,  lambda_grid={\"alpha\": np.logspace(-2, 2, 10)}, **stabl_base_kwargs)\n",
    "\n",
    "stabl_en = clone(stabl_lasso).set_params(\n",
    "    base_estimator=en,\n",
    "    lambda_grid=en_grid\n",
    ")\n",
    "\n",
    "stabl_rf = clone(stabl_lasso).set_params(\n",
    "    base_estimator=rf,\n",
    "    lambda_grid=rf_grid\n",
    ")\n",
    "\n",
    "stabl_xgb = clone(stabl_lasso).set_params(\n",
    "    base_estimator=xgb,\n",
    "    lambda_grid=xgb_grid\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 4.¬†Dictionnaire des estimateurs\n",
    "# ===============================\n",
    "estimators = {\n",
    "    \"lasso\":       lasso_cv,\n",
    "#    \"en\":          en_cv,\n",
    "    \"rf\":          rf_cv,\n",
    "    \"xgb\":         xgb_cv,\n",
    "    \"stabl_lasso\": stabl_lasso,\n",
    "#    \"stabl_en\":    stabl_en,\n",
    "    \"stabl_rf\":    stabl_rf,\n",
    "    \"stabl_xgb\":   stabl_xgb,\n",
    "}\n",
    "\n",
    "models = [\n",
    "    \"Lasso\",\n",
    "#    \"ElasticNet\",\n",
    "    \"RandomForest\",\n",
    "    \"XGBoost\",\n",
    "    \"STABL Lasso\",\n",
    "#    \"STABL ElasticNet\",\n",
    "    \"STABL RandomForest\",\n",
    "    \"STABL XGBoost\",\n",
    "]\n",
    "\n",
    "# juste apr√®s avoir construit ton dict estimators, ajoute :\n",
    "estimators[\"en\"]        = estimators[\"xgb\"]        # placeholder vide\n",
    "estimators[\"stabl_en\"]  = estimators[\"stabl_xgb\"]  # placeholder vide\n",
    "\n",
    "estimators[\"cb\"]        = estimators[\"xgb\"]        # placeholder vide\n",
    "estimators[\"stabl_cb\"]  = estimators[\"stabl_xgb\"]  # placeholder vide\n",
    "\n",
    "estimators[\"lgb\"]        = estimators[\"xgb\"]        # placeholder vide\n",
    "estimators[\"stabl_lgb\"]  = estimators[\"stabl_xgb\"]  # placeholder vide\n",
    "\n",
    "# ===============================\n",
    "# 5.¬†Chargement des donn√©es\n",
    "# ===============================\n",
    "features_path = \"/Users/noeamar/Documents/Stanford/data/olivier_data/ina_13OG_df_168_filtered_allstim_new.csv\"  #¬†√† adapter\n",
    "outcome_path  = \"/Users/noeamar/Documents/Stanford/data/olivier_data/outcome_table_all_pre.csv\"   #¬†√† adapter\n",
    "\n",
    "X_train, X_val, y_train, y_val, groups, task_type = load_onset_data(features_path, outcome_path)\n",
    "\n",
    "# ===============================\n",
    "# 6.¬†Lancement du benchmark\n",
    "# ===============================\n",
    "save_path = Path(\"./benchmark_results_time/KO\")\n",
    "if save_path.exists():\n",
    "    shutil.rmtree(save_path)\n",
    "\n",
    "print(\"\\nüöÄ¬†Lancement du benchmark STABL¬†‚Ä¶\")\n",
    "\n",
    "multi_omic_stabl_cv(\n",
    "    data_dict=X_train,\n",
    "    y=y_train,\n",
    "    outer_splitter=outer_cv,\n",
    "    estimators=estimators,\n",
    "    task_type=task_type,\n",
    "    save_path=str(save_path),\n",
    "    outer_groups=groups,\n",
    "    early_fusion=False,\n",
    "    late_fusion=True,\n",
    "    n_iter_lf=1000,\n",
    "    models=models,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ¬†Benchmark termin√©. R√©sultats dans\", save_path)\n",
    "\n",
    "# temps = 5h30 a peu pres\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
